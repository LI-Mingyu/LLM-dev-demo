import numpy as np
import streamlit as st
import tiktoken
from openai import OpenAI  
import logging
from redis import Redis
from redis.commands.search.query import Query
import os
from datetime import date, datetime, timedelta
import json

# Define a function to get the session id and the remote ip 
# Caution: this function is implemented in a hacky way and may break in the future
from streamlit import runtime
from streamlit.runtime.scriptrunner import get_script_run_ctx

LLM = "qwen-max"
VECTOR_DIM = 1024
DISTANCE_METRIC = "COSINE"  
INDEX_NAME = "AliyunQA"

client = OpenAI(
    api_key=os.getenv("DASHSCOPE_API_KEY"),  # ç¯å¢ƒå˜é‡åç§°å˜æ›´
    base_url="https://dashscope.aliyuncs.com/compatible-mode/v1"
)

# Helper functions
def json_gpt(input: str):
    completion = client.chat.completions.create(  # ä½¿ç”¨å®¢æˆ·ç«¯å®ä¾‹
        model=LLM,
        messages=[
            {"role": "system", "content": "Output only valid JSON"},
            {"role": "user", "content": input},
        ],
        temperature=0.2,
    )
    text = completion.choices[0].message.content
    parsed = json.loads(text)

    return parsed


# The streamlit script starts here
logging.info("Starting Streamlit script ...")

# Prepare to connect to Redis
redis_host = os.getenv('REDIS_HOST', 'localhost')  # default to 'localhost' if not set
redis_port = os.getenv('REDIS_PORT', '6379')  # default to '6379' if not set
redis_db = os.getenv('REDIS_DB', '0')  # default to '0' if not set. RediSearch only operates on the default (0) db
 # Instantiates a Redis client. decode_responses=False to avoid decoding the returned embedding vectors
r = Redis(host=redis_host, port=redis_port, db=redis_db, decode_responses=False)


st.set_page_config(
    page_title="é˜¿é‡Œäº‘è¿ç»´åŠ©æ‰‹",
    page_icon="ğŸ¤–",
)
st.subheader("é˜¿é‡Œäº‘è¿ç»´åŠ©æ‰‹")

if "messages" not in st.session_state.keys():
    # Initialize the session_state.messages
    st.session_state.messages = [{"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªé˜¿é‡Œäº‘è¿ç»´åŠ©æ‰‹ã€‚"}]
else: # Since streamlit script is executed every time a widget is changed, this "else" is not necessary, but improves readability
    # Display chat messages
    for message in st.session_state.messages:
        if message["role"] == "user" or message["role"] == "assistant":
            with st.chat_message(message["role"]):
                st.write(message["content"])

# User-provided prompt
if user_prompt := st.chat_input('åœ¨æ­¤è¾“å…¥æ‚¨çš„é—®é¢˜'):
    logging.info(f"User's prompt: {user_prompt}")
    with st.chat_message("user"):
        st.write(user_prompt)
        st.session_state.messages.append({"role": "user", "content": user_prompt})
    with st.chat_message("assistant"):
        with st.spinner("äººå·¥æ™ºèƒ½æ­£åœ¨æ€è€ƒ..."):
            QUERY_GEN_PROMPT = f"""
You are an Aliyun operations assistant
You have access to a search API that returns troubleshooting articles.
Generate search query extracting key words from the user's question.

User question: {user_prompt}

Format: {{"searchQuery": "search query"}}"""
            query_str = json_gpt(QUERY_GEN_PROMPT)["searchQuery"]
            logging.info(f"Generated query: {query_str}")
            try:
                query_embedding = client.embeddings.create(input=query_str, model="text-embedding-v3", dimensions=1024, encoding_format="float")
                query_vec = np.array(query_embedding.data[0].embedding, dtype=np.float32).tobytes()
                # Prepare the query
                query_base = (Query("*=>[KNN 2 @md_embedding $vec as score]").sort_by("score").return_fields("score", "url", "md").dialect(2))
                query_param = {"vec": query_vec}
                query_results = r.ft(INDEX_NAME).search(query_base, query_param).docs
                result_md = query_results[0].md + "\n\n" + query_results[1].md
            except Exception as e:
                logging.error(f"Error querying Reids with embedding: {e}")
                st.error("æ— æ³•æœç´¢ç­”æ¡ˆï¼Œè¿™å¾ˆå¯èƒ½æ˜¯ç³»ç»Ÿæ•…éšœå¯¼è‡´ï¼Œè¯·è”ç³»æˆ‘çš„ä¸»äººã€‚")
                st.stop()
            logging.info(f"Search results: ({query_results[0].score}){query_results[0].md[:20]}".replace("\n", "") +
                        "..." + f"({query_results[1].score}){query_results[1].md[:20]}".replace("\n", "") + "...")
            st.session_state.messages.append({"role": "user", "content": f"""è¯·æ ¹æ®æœç´¢ç»“æœå›ç­”useråœ¨å‰é¢é‡åˆ°çš„é—®é¢˜ã€‚æ³¨æ„ï¼Œè¯·åŠ¡å¿…é¦–å…ˆä¾èµ–æœç´¢ç»“æœï¼Œè€Œä¸æ˜¯ä½ è‡ªå·±å·²æœ‰çš„çŸ¥è¯†ã€‚å¦‚æœæœç´¢ç»“æœä¸­åŒ…å«äº†å…·ä½“æ“ä½œæ­¥éª¤ï¼Œä¹Ÿè¯·æ®æ­¤ç»™ç”¨æˆ·å…·ä½“æ“ä½œæŒ‡å¼•ã€‚
æœç´¢ç»“æœï¼š\n{result_md}"""})
            test_messages = st.session_state.messages.copy()
            try:
                # æµå¼å“åº”å¤„ç†
                llm_response = client.chat.completions.create(
                    model=LLM,
                    messages=st.session_state.messages,
                    temperature=0.5,
                    stream=True
                )
                resp_display = st.empty()
                collected_resp_content = ""
                for chunk in llm_response:
                    if chunk.choices[0].finish_reason is None:  
                        content = chunk.choices[0].delta.content 
                        if content:
                            collected_resp_content += content
                            resp_display.write(collected_resp_content)

            # ä¿®æ”¹6: é”™è¯¯å¤„ç†é€‚é…
            except Exception as e:
                logging.error(f"é˜¿é‡Œäº‘APIé”™è¯¯: {e}")
                st.error('æœåŠ¡æš‚æ—¶ä¸å¯ç”¨ï¼Œè¯·ç¨åé‡è¯•æˆ–è”ç³»ç®¡ç†å‘˜')
                st.stop()
            logging.info(f"AI's response: {collected_resp_content[:50]}".replace("\n", "") + "..." + f"{collected_resp_content[-50:]}".replace("\n", ""))

    # Add the generated msg to session state
    st.session_state.messages[-1] = {"role": "assistant", "content": collected_resp_content}
    st.write(f"\nå‚è€ƒæ–‡æ¡£ï¼š\n\n{query_results[0].url}\n\n{query_results[1].url}")

st.write("**æ³¨æ„ï¼šäººå·¥æ™ºèƒ½ç”Ÿæˆå†…å®¹ä»…ä¾›å‚è€ƒã€‚**")
logging.info("Streamlit script ended.")
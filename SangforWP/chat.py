import numpy as np
import streamlit as st
import tiktoken
import openai 
import logging
# Milvus ç›¸å…³çš„åº“å¯¼å…¥
from pymilvus import (
    connections,  # ç”¨äºè¿æ¥ Milvus æ•°æ®åº“
    Collection,  # ç”¨äºæ“ä½œ Milvus é›†åˆ
    CollectionSchema,  # å®šä¹‰é›†åˆçš„ schema
    FieldSchema,  # å®šä¹‰å­—æ®µ
    DataType,  # æ•°æ®ç±»å‹
    utility  # ç”¨äºæ‰§è¡Œç‰¹å®šçš„æ•°æ®åº“å·¥å…·å‡½æ•°ï¼Œæ¯”å¦‚æ£€æŸ¥é›†åˆæ˜¯å¦å­˜åœ¨
)

import os
from datetime import date, datetime, timedelta
import json

# Define a function to get the session id and the remote ip 
# Caution: this function is implemented in a hacky way and may break in the future
from streamlit import runtime
from streamlit.runtime.scriptrunner import get_script_run_ctx

GPT_MODEL = "gpt-4"
VECTOR_DIM = 1536 
DISTANCE_METRIC = "COSINE"  
INDEX_NAME = "SangforWP"

# Helper functions
def json_gpt(input: str):
    completion = openai.ChatCompletion.create(
        model=GPT_MODEL,
        messages=[
            {"role": "system", "content": "Output only valid JSON"},
            {"role": "user", "content": input},
        ],
        temperature=0.2,
    )

    text = completion.choices[0].message.content
    parsed = json.loads(text)

    return parsed

# å®šä¹‰Milvusçš„ç›¸å…³å‡½æ•°

# The streamlit script starts here
logging.info("Starting Streamlit script ...")

# è¿æ¥Milvus
# ä»ç¯å¢ƒå˜é‡è·å– Milvus æœåŠ¡å™¨é…ç½®ï¼Œæˆ–ä½¿ç”¨é»˜è®¤å€¼
milvus_host = os.getenv('MILVUS_HOST', 'localhost')       # é»˜è®¤ä¸º localhost
milvus_port = os.getenv('MILVUS_PORT', '19530')           # Milvus é»˜è®¤ç«¯å£ä¸º 19530

# å»ºç«‹è¿æ¥
connections.connect(alias="default", host=milvus_host, port=milvus_port)


st.set_page_config(
    page_title="ä¼šè¯´è¯çš„ç™½çš®ä¹¦",
    page_icon="ğŸ“š",
)
st.subheader("ğŸ“šä¼šè¯´è¯çš„ç™½çš®ä¹¦")
st.write("æ‚¨å¥½ï¼æˆ‘æ˜¯ã€Šæ·±ä¿¡æœè¶…èåˆå¯é æ€§ç™½çš®ä¹¦ã€‹ï¼Œæœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®åˆ°æ‚¨çš„ï¼Ÿ")

if "messages" not in st.session_state.keys():
    # Initialize the session_state.messages
    st.session_state.messages = [{"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªæœ‰ITç³»ç»Ÿä¸“ä¸šçŸ¥è¯†çš„äººå·¥æ™ºèƒ½åŠ©æ‰‹ï¼Œå°¤å…¶ç²¾é€šè¶…èåˆçš„å¯é æ€§æŠ€æœ¯ã€‚ä½ çš„ä¸»è¦èŒè´£æ˜¯å›ç­”ç”¨æˆ·å…³äºæ·±ä¿¡æœè¶…èåˆç³»ç»Ÿçš„å¯é æ€§æ–¹é¢çš„é—®é¢˜ã€‚"}]
else: # Since streamlit script is executed every time a widget is changed, this "else" is not necessary, but improves readability
    # Display chat messages
    for message in st.session_state.messages:
        if message["role"] == "user" or message["role"] == "assistant":
            with st.chat_message(message["role"]):
                st.write(message["content"])

# Prepare the api_key to call OpenAI
openai_api_key = os.getenv("OPENAI_API_KEY")
if not openai_api_key:
    logging.error("KEY is not set.")
    st.error('KEYæœªè®¾ç½®ï¼Œè¯·è”ç³»æˆ‘çš„ä¸»äººã€‚')
    st.stop() 
else:
    openai.api_key = openai_api_key

# User-provided prompt
if user_prompt := st.chat_input('åœ¨æ­¤è¾“å…¥æ‚¨çš„é—®é¢˜'):
    logging.info(f"User's prompt: {user_prompt}")
    with st.chat_message("user"):
        st.write(user_prompt)
        st.session_state.messages.append({"role": "user", "content": user_prompt})
    with st.chat_message("assistant"):
        with st.spinner("äººå·¥æ™ºèƒ½æ­£åœ¨æ€è€ƒ..."):
            QUERY_GEN_PROMPT = f"""
You are an AI assistant that answers questions about Sangfor HCI reliability.
You have access to a search API that returns troubleshooting articles.
Generate search query extracting key words from the user's question.

User question: {user_prompt}
Format: {{"searchQuery": "search query"}}"""
            
            query_str = json_gpt(QUERY_GEN_PROMPT)["searchQuery"] # ä½¿ç”¨GPTæ¥ç”ŸæˆæŸ¥è¯¢å­—ç¬¦ä¸²
            logging.info(f"Generated query: {query_str}")
            try:
                # ç”ŸæˆæŸ¥è¯¢çš„å‘é‡è¡¨ç¤º
                query_embedding = openai.Embedding.create(input=query_str, model="text-embedding-ada-002")["data"][0]["embedding"]
                query_vec = np.array(query_embedding).astype(np.float32).tolist()

                # ä½¿ç”¨ Milvus æ‰§è¡Œå‘é‡æœç´¢
                collection = Collection(name="pdf_page_collection")  # åŠ è½½å·²å­˜åœ¨çš„é›†åˆ
                search_params = {"metric_type": "L2", "params": {"nprobe": 16}}
                search_results = collection.search([query_vec], "page_embedding", search_params, limit=2, output_fields=["page_num", "content"])

                # å¤„ç†æœç´¢ç»“æœ
                result_contents = [doc.entity.get('content') for doc in search_results[0]]
                result_str = "\n\n------another page------\n\n".join(result_contents)
                st.session_state.messages.append({"role": "user", "content": "è¯·æ ¹æ®æœç´¢ç»“æœå›ç­”ç”¨æˆ·åœ¨å‰é¢é‡åˆ°çš„é—®é¢˜â€¦â€¦\n" + result_str})

            except Exception as e:
                logging.error(f"Error during Milvus processing: {e}")
                st.error("æ— æ³•æœç´¢ç­”æ¡ˆï¼Œè¿™å¾ˆå¯èƒ½æ˜¯ç³»ç»Ÿæ•…éšœå¯¼è‡´ï¼Œè¯·è”ç³»æˆ‘çš„ä¸»äººã€‚")
                st.stop()

            # ç¡®ä¿æœç´¢ç»“æœä¸ä¸ºç©ºå¹¶ä¸”è‡³å°‘æœ‰ä¸€é¡¹ç»“æœ
            if len(search_results) > 0:
                first_hits = search_results[0]  # è·å–ç¬¬ä¸€ä¸ªæŸ¥è¯¢çš„ç»“æœï¼ˆè¿™é‡Œå‡è®¾åªæœ‰ä¸€ä¸ªæŸ¥è¯¢ï¼‰
                # è®°å½•æ—¥å¿—ï¼Œæ˜¾ç¤ºæœ€å¤šå‰ä¸¤ä¸ªç»“æœçš„ä¿¡æ¯
                if len(first_hits) > 1:
                    logging.info(f"Search results: ({first_hits[0].score}){first_hits[0].entity.get('content')[:20]}..." +
                                f"({first_hits[1].score}){first_hits[1].entity.get('content')[:20]}...")
                elif len(first_hits) == 1:
                    logging.info(f"Search result: ({first_hits[0].score}){first_hits[0].entity.get('content')[:20]}...")
                else:
                    logging.info("No results found.")
            else:
                logging.error("No results from Milvus search.")
            
            st.session_state.messages.append({"role": "user", "content": f"""è¯·æ ¹æ®æœç´¢ç»“æœå›ç­”useråœ¨å‰é¢é‡åˆ°çš„é—®é¢˜ã€‚æ³¨æ„ï¼Œè¯·åŠ¡å¿…é¦–å…ˆä¾èµ–æœç´¢ç»“æœï¼Œè€Œä¸æ˜¯ä½ è‡ªå·±å·²æœ‰çš„çŸ¥è¯†ã€‚å¦‚æœæœç´¢ç»“æœä¸­åŒ…å«äº†å…·ä½“æ“ä½œæ­¥éª¤ï¼Œä¹Ÿè¯·æ®æ­¤ç»™ç”¨æˆ·å…·ä½“æ“ä½œæŒ‡å¼•ã€‚
æœç´¢ç»“æœï¼š\n{result_str}"""})
            
            test_messages = st.session_state.messages.copy() # make a copy of the messages for testing & debugging
            try:
                gpt_response = openai.ChatCompletion.create(
                        model=GPT_MODEL,
                        messages=st.session_state.messages,
                        temperature=0.5,
                        stream=True)
                resp_display = st.empty()
                collected_resp_content = ""
                for chunk in gpt_response:
                    if not chunk['choices'][0]['finish_reason']:
                        collected_resp_content += chunk['choices'][0]['delta']['content']
                        resp_display.write(collected_resp_content)
            except Exception as e:
                logging.error(f"Error generating response from OpenAI: {e}")
                st.error('AIæ²¡æœ‰å“åº”ï¼Œå¯èƒ½æ˜¯å› ä¸ºæˆ‘ä»¬æ­£åœ¨ç»å†è®¿é—®é«˜å³°ï¼Œè¯·ç¨ååˆ·æ–°é¡µé¢é‡è¯•ã€‚å¦‚æœé—®é¢˜ä»ç„¶å­˜åœ¨ï¼Œè¯·è”ç³»æˆ‘çš„ä¸»äººã€‚')
                st.stop() 
            logging.info(f"AI's response: {collected_resp_content[:50]}".replace("\n", "") + "..." + f"{collected_resp_content[-50:]}".replace("\n", ""))

    # Count the number of tokens used
    tokenizer = tiktoken.encoding_for_model(GPT_MODEL)
    # Because of ChatGPT's message format, there are 4 extra tokens for each message
    # Ref: https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb
    n_token_input = 0
    for message in st.session_state.messages:
        n_token_input += len(tokenizer.encode(message["content"])) + 4 
    n_token_input += 3 # every reply is primed with <|start|>assistant<|message|>
    n_token_output = len(tokenizer.encode(collected_resp_content))
    logging.info(f"Token usage: {n_token_input} -> {n_token_output}")

    # Add the generated msg to session state
    st.session_state.messages[-1] = {"role": "assistant", "content": collected_resp_content}

    
    # ç¡®ä¿æœ‰è¶³å¤Ÿçš„ç»“æœ
    if len(search_results) > 0 and len(search_results[0]) > 1:
        first_two_hits = search_results[0]  # ç¬¬ä¸€ä¸ªæŸ¥è¯¢çš„ç»“æœ
        st.write(f"\nå‚è€ƒç™½çš®ä¹¦ï¼šç¬¬{first_two_hits[0].entity.get('page_num')}é¡µå’Œç¬¬{first_two_hits[1].entity.get('page_num')}é¡µ")
    else:
        st.write("æœç´¢ç»“æœä¸è¶³ä»¥æä¾›ç›¸åº”çš„é¡µé¢å·ã€‚")
    
    # # æ£€æµ‹æœç´¢ç»“æœ, ç”¨äºæµ‹è¯•
    # if search_results and len(search_results[0]) > 0:
    #     for hit in search_results[0]:  # éå†ç¬¬ä¸€ä¸ªï¼ˆæˆ–å”¯ä¸€çš„ï¼‰æŸ¥è¯¢ç»“æœä¸­çš„å‘½ä¸­
    #         logging.info(f"Document ID: {hit.id}, Score: {hit.score}, Content: {hit.entity.get('content')[:50]}")  # æ—¥å¿—è®°å½•å‰50ä¸ªå­—ç¬¦
    # else:
    #     logging.error("No results found or search failed.")

st.write("**æ³¨æ„ï¼šäººå·¥æ™ºèƒ½ç”Ÿæˆå†…å®¹ä»…ä¾›å‚è€ƒã€‚**")
logging.info("Streamlit script ended.")
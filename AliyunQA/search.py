import numpy as np
import streamlit as st
import tiktoken
import openai 
import logging
from redis import Redis
from redis.commands.search.query import Query
import os
from datetime import date, datetime, timedelta
import json

# Define a function to get the session id and the remote ip 
# Caution: this function is implemented in a hacky way and may break in the future
from streamlit import runtime
from streamlit.runtime.scriptrunner import get_script_run_ctx

GPT_MODEL = "gpt-4"
VECTOR_DIM = 1536 
DISTANCE_METRIC = "COSINE"  
INDEX_NAME = "AliyunQA"

# Helper functions
def json_gpt(input: str):
    completion = openai.ChatCompletion.create(
        model=GPT_MODEL,
        messages=[
            {"role": "system", "content": "Output only valid JSON"},
            {"role": "user", "content": input},
        ],
        temperature=0.2,
    )

    text = completion.choices[0].message.content
    parsed = json.loads(text)

    return parsed

# Set up logging
logging.basicConfig(level=logging.INFO,
                    format='%(asctime)s - %(levelname)s - %(message)s',
                    datefmt='%Y-%m-%d %H:%M:%S')

# The streamlit script starts here
logging.info("Starting Streamlit script ...")

# Prepare to connect to Redis
redis_host = os.getenv('REDIS_HOST', 'localhost')  # default to 'localhost' if not set
redis_port = os.getenv('REDIS_PORT', '6379')  # default to '6379' if not set
redis_db = os.getenv('REDIS_DB', '0')  # default to '0' if not set. RediSearch only operates on the default (0) db
 # Instantiates a Redis client. decode_responses=False to avoid decoding the returned embedding vectors
r = Redis(host=redis_host, port=redis_port, db=redis_db, decode_responses=False)


st.set_page_config(
    page_title="é˜¿é‡Œäº‘è¿ç»´åŠ©æ‰‹",
    page_icon="ğŸ¤–",
)
st.subheader("é˜¿é‡Œäº‘è¿ç»´åŠ©æ‰‹")

user_prompt = st.text_input("è¯·è¾“å…¥æ‚¨çš„é—®é¢˜ï¼š","", key="input")

if st.button('æé—®', key='generationSubmit'):
    with st.spinner("äººå·¥æ™ºèƒ½æ­£åœ¨æ€è€ƒ..."):
        QUERY_GEN_PROMPT = f"""
You are an Aliyun operations assistant
You have access to a search API that returns troubleshooting articles.
Generate search query extracting key words from the user's question.

User question: {user_prompt}

Format: {{"searchQuery": "search query"}}"""
        
    query_str = json_gpt(QUERY_GEN_PROMPT)["searchQuery"]
    logging.info(f"Generated query: {query_str}")

    with st.spinner("äººå·¥æ™ºèƒ½æ­£åœ¨æœç´¢..."):
        query_embedding = openai.Embedding.create(input=query_str, model="text-embedding-ada-002")["data"][0]["embedding"]
        query_vec = np.array(query_embedding).astype(np.float32).tobytes()
        # Prepare the query
        query_base = (Query("*=>[KNN 2 @md_embedding $vec as score]").sort_by("score").return_fields("score", "url", "md").dialect(2))
        query_param = {"vec": query_vec}
        query_results = r.ft(INDEX_NAME).search(query_base, query_param).docs
        result_md = query_results[0].md + "\n\n" + query_results[1].md
    logging.info(f"Search results: ({query_results[0].score}){query_results[0].md[:20]}".replace("\n", "") +
                "..." + f"({query_results[1].score}){query_results[1].md[:20]}".replace("\n", "") + "...")

    system_prompt = "ä½ æ˜¯ä¸€ä¸ªé˜¿é‡Œäº‘è¿ç»´åŠ©æ‰‹ã€‚è¯·æ ¹æ®æœç´¢ç»“æœå›ç­”ç”¨æˆ·æé—®ï¼Œæ³¨æ„ï¼Œè¯·åŠ¡å¿…é¦–å…ˆä¾èµ–æœç´¢ç»“æœï¼Œè€Œä¸æ˜¯ä½ è‡ªå·±å·²æœ‰çš„çŸ¥è¯†ã€‚å¦‚æœæœç´¢ç»“æœä¸­åŒ…å«äº†å…·ä½“æ“ä½œæ­¥éª¤ï¼Œä¹Ÿè¯·æ®æ­¤ç»™ç”¨æˆ·å…·ä½“æ“ä½œæŒ‡å¼•ã€‚"
    messages = [{"role": "system", "content": system_prompt},
                {"role": "user", "content": "ç”¨æˆ·æé—®ï¼š" + query_str},
                {"role": "user", "content": "æœç´¢ç»“æœï¼š" + result_md}]
    
    response = openai.ChatCompletion.create(
                        model=GPT_MODEL,
                        messages=messages,
                        temperature=0.5,
                        stream=True
                    )
    collected_resp_content = ""
    resp_display = st.empty()
    for chunk in response:
        if not chunk['choices'][0]['finish_reason']:
            collected_resp_content += chunk['choices'][0]['delta']['content']
            resp_display.write(collected_resp_content)
    logging.info(f"AI's response: {collected_resp_content[:50]}".replace("\n", "") + "..." + f"{collected_resp_content[-50:]}".replace("\n", ""))

st.write("**æ³¨æ„ï¼šäººå·¥æ™ºèƒ½ç”Ÿæˆå†…å®¹ä»…ä¾›å‚è€ƒã€‚**")
logging.info("Streamlit script ended.")